{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, yaml\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting the credentials required for OPEN AI/ TOGETHER\n",
    "with open(\"credential.yaml\", 'r') as file:\n",
    "    credentials= yaml.load(file, Loader=yaml.FullLoader)\n",
    "\n",
    "os.environ['OPENAI_API_KEY']= credentials['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Building client \n",
    "client= OpenAI() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating responses based on the user messages\n",
    "def complete_response(prompt):\n",
    "    response= client.chat.completions.create(        \n",
    "        model='gpt-3.5-turbo',\n",
    "        messages= [{\"role\": \"system\",\"content\" : \"You are a helpful assistant to generate responses on the given prompt.\"},\n",
    "                   {\"role\": \"user\", \"content\": prompt}\n",
    "                   ],\n",
    "        temperature=0.5,\n",
    "        max_tokens=500\n",
    "    )\n",
    "    return str(response.choices[0].message.content)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# zero shot prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt='''Classify the below sentence in to neutral, happy or sad \n",
    "Text : I went to a wonderful journey to UK \n",
    "Sentiment:'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=''' Summarize the below content in to 2 sentences.\n",
    "Content : While large language models (colloquially termed \"AI chatbots\" in some contexts) can be very useful, machine-generated text (much like human-generated text) can contain errors or flaws, or be outright useless.Specifically, asking an LLM to \"write a Wikipedia article\" can sometimes cause the output to be outright fabrication, complete with fictitious references. It may be biased, may libel living people, or may violate copyrights. Thus, all text generated by LLMs should be independently verified by editors before being used in Wikipedia articles.Editors who are not fully aware of these risks and are not able to overcome the limitations of these tools should not edit with their assistance. LLMs should not be used for tasks with which the editor does not have substantial familiarity. Their outputs should be rigorously scrutinized for compliance with all applicable policies. In any case, editors should avoid publishing content on Wikipedia obtained by asking LLMs to write original content. Even if such content has been heavily edited, alternatives that don't use machine-generated content are preferable. As with all their edits, an editor is fully responsible for their LLM-assisted edits.Wikipedia is not a testing ground for LLMs. Using LLMs to write one's talk page comments or edit summaries is strongly discouraged. Furthermore, LLM use to generate or modify text should be mentioned in the edit summary (even if their terms of service do not require it)\n",
    "summary : '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "output=complete_response(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "357\n"
     ]
    }
   ],
   "source": [
    "print(len(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('The content highlights the potential risks associated with using large '\n",
      " 'language models (LLMs), also known as \"AI chatbots,\" for generating text. It '\n",
      " 'warns that machine-generated text, like human-generated text, can contain '\n",
      " 'errors, biases, libelous content, or copyright violations. The '\n",
      " 'recommendation is to have all text generated by LLMs independently verified '\n",
      " 'by editors before using it in Wikipedia articles. Editors are advised to be '\n",
      " 'cautious, avoid using LLMs for tasks they are not familiar with, and ensure '\n",
      " 'compliance with policies. It emphasizes that editors should take full '\n",
      " 'responsibility for any edits made with LLM assistance and discourages using '\n",
      " 'LLMs for talk page comments or edit summaries on Wikipedia.')\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "pprint.pprint(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Few Shot Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=''' Identify the sentiment of the below content. sentiment need to be positive and negative only\n",
    "content = \"The movie was fantastic! Great acting\"\n",
    "Sentiment= Positive\n",
    "\n",
    "context = \"The plot of the story was not upto the standard.\"\n",
    "sentiment = Negative\n",
    "\n",
    "context = \"The lightning of the stage was okay\"\n",
    "sentiment= positive\n",
    "\n",
    "context = \"the costumes were fine. The huge impact\"\n",
    "sentiment\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sentiment for the last content is positive.\n"
     ]
    }
   ],
   "source": [
    "print(complete_response(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt='''You are supposed to Answer to the  questions based on the context\n",
    "context= \"Mars is the fourth planet from the Sun.  It is a dusty, cold, desert world with a very thin atmosphere. Mars is also a dynamic planet with seasons, polar ice caps, canyons, extinct volcanoes, \n",
    "and evidence that it was even more active in the past.\"\n",
    "\n",
    "Question : Is mars a hot planet?\n",
    "response : no\n",
    "\n",
    "Question : Does mars have water\n",
    "response: No\n",
    "\n",
    "Question : Does mars has winter\n",
    "response : yes\n",
    "\n",
    "Question : Does mars has volcanoes\n",
    "response?\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Yes, Mars has extinct volcanoes.\n"
     ]
    }
   ],
   "source": [
    "print(complete_response(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
